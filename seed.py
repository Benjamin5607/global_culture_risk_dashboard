import os
import json
import requests
import time
import random
from datetime import datetime

# ê¹ƒí—ˆë¸Œ Secretsì—ì„œ í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤
API_KEY = os.environ.get("GROQ_API_KEY")

def get_current_date():
    return datetime.now().strftime("%Y-%m-%d")

def generate_massive_data():
    print("ğŸ­ GitHub Cloud Factory: Five Eyes (US/UK/CA/AU/NZ) Mode Started...")

    if not API_KEY:
        print("âŒ Error: GROQ_API_KEY not found in Secrets.")
        return

    # 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    try:
        with open("data.json", "r", encoding="utf-8") as f:
            current_data = json.load(f)
        print(f"ğŸ“‚ Loaded existing data: {len(current_data)} items")
    except:
        current_data = []
        print("ğŸ“‚ No existing data. Starting fresh.")

    existing_terms = {item['term'].lower() for item in current_data}

    # ==========================================
    # ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ë¯¸êµ­, ì˜êµ­, ìºë‚˜ë‹¤, í˜¸ì£¼, ë‰´ì§ˆëœë“œ ì§‘ì¤‘)
    # ==========================================
    prompts = []
    target_countries = ["USA", "UK", "Canada", "Australia", "New Zealand"]
    
    # ì „ëµ: êµ­ê°€ë³„ + ì•ŒíŒŒë²³ë³„ ë¶„í•  ì •ë³µ
    alphabet_chunks = ["ABCDE", "FGHIJ", "KLMNO", "PQRST", "UVWXYZ"]
    
    for country in target_countries:
        # ì•ŒíŒŒë²³ë³„ ìŠ¬ë­ ì°¾ê¸°
        for chunk in alphabet_chunks:
            prompts.append(f"Gen Z internet slang in {country} starting with letters {chunk}")
        
        # êµ­ê°€ë³„ íŠ¹ìˆ˜ ì£¼ì œ
        prompts.append(f"Political dog whistles used in {country}")
        prompts.append(f"Controversial influencers in {country} (2024-2025)")
        prompts.append(f"Corporate buzzwords specific to {country}")

    # ê³µí†µ ì£¼ì œ
    common_topics = [
        "Incel and Manosphere terminology (English)",
        "Gaming and Twitch chat slang (Western)",
        "Algospeak words on TikTok (English)",
        "Crypto slang"
    ]
    prompts.extend(common_topics)

    random.shuffle(prompts) # ìˆœì„œ ì„ê¸°

    # ==========================================
    # ê³µì¥ ê°€ë™ (ê¹ƒí—ˆë¸Œ ì•¡ì…˜ ì‹œê°„ ì œí•œ ê³ ë ¤í•˜ì—¬ ìµœëŒ€ 15ë²ˆ ë°°ì¹˜ë§Œ ì‹¤í–‰)
    # ==========================================
    # ë¡œì»¬ê³¼ ë‹¬ë¦¬ ê¹ƒí—ˆë¸ŒëŠ” ë„ˆë¬´ ì˜¤ë˜ ëŒë©´ ê°•ì œ ì¢…ë£Œë  ìˆ˜ ìˆì–´ì„œ
    # í•œ ë²ˆ ì‹¤í–‰ì— 15ë²ˆ ì§ˆë¬¸(ì•½ 100~120ê°œ ìƒì‚°) ì •ë„ë¡œ ì œí•œí•˜ëŠ” ê²Œ ì•ˆì „í•©ë‹ˆë‹¤.
    max_batches = 15 
    
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}

    for i, specific_topic in enumerate(prompts[:max_batches]):
        print(f"\nğŸ”„ Batch [{i+1}/{max_batches}] Topic: '{specific_topic}'")

        system_prompt = f"""
        List 8 distinct real-world terms related to "{specific_topic}".
        Target Countries: USA, UK, Canada, Australia, New Zealand ONLY.
        Focus on trends from 2024-2026.
        
        Output JSON object with key "items".
        Schema: term, group, country (list), category, risk_level, trend_score (40-99), status ('Active'), first_detected, last_updated, context: {{en, ko, ja}}
        """

        payload = {
            "model": "llama-3.3-70b-versatile",
            "messages": [
                {"role": "system", "content": "Output JSON only."},
                {"role": "user", "content": system_prompt}
            ],
            "response_format": {"type": "json_object"}
        }

        try:
            response = requests.post(url, headers=headers, json=payload)
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                batch_data = json.loads(content).get('items', [])
                
                added = 0
                for item in batch_data:
                    term_key = item['term'].lower().strip()
                    if term_key not in existing_terms:
                        item['term'] = item['term'].strip()
                        item['last_updated'] = get_current_date()
                        current_data.append(item)
                        existing_terms.add(term_key)
                        added += 1
                print(f"   âœ… Added {added} items.")
            else:
                print(f"   âŒ API Error: {response.text}")
        except Exception as e:
            print(f"   âš ï¸ Exception: {e}")

        time.sleep(1) # 1ì´ˆ íœ´ì‹

    # ì €ì¥
    print(f"\nğŸ’¾ Saving to data.json... Total items: {len(current_data)}")
    with open("data.json", "w", encoding="utf-8") as f:
        json.dump(current_data, f, indent=4, ensure_ascii=False)

if __name__ == "__main__":
    generate_massive_data()
